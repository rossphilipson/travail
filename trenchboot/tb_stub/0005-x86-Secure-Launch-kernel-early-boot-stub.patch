From fb7c468517670945868313a6ce8b01fa8ea4a951 Mon Sep 17 00:00:00 2001
From: Ross Philipson <ross.philipson@oracle.com>
Date: Fri, 26 Oct 2018 15:53:24 -0400
Subject: [PATCH 5/8] x86: Secure Launch kernel early boot stub

The Secure Launch (SL) stub provides the entry point for Intel TXT (and
later AMD SKINIT) to vector to during the late launch. The symbol
sl_stub_entry is that entry point and is conveyed launching code using
the MLE (Measured Launch Environment) header represented by the symbol
sl_mle_header. The offset of the MLE is written to the boot params at
build time. The routine sl_stub contains the very early late launch setup
code responsible for setting up the basic environment to allow the normal
kernel startup_32 code to proceed. It is also responsible for properly
waking and handling the APs on Intel platforms. The routine sl_main which
runs after entering 64b mode is responsible for measuring configuration
information before it is used like the boot params, the kernel command
line, the TXT heap, etc.

Signed-off-by: Ross Philipson <ross.philipson@oracle.com>
---
 arch/x86/boot/compressed/Makefile  |   2 +-
 arch/x86/boot/compressed/head_64.S |  33 +++
 arch/x86/boot/compressed/sl_main.c | 186 ++++++++++++++
 arch/x86/boot/compressed/sl_stub.S | 505 +++++++++++++++++++++++++++++++++++++
 arch/x86/kernel/asm-offsets.c      |  10 +
 5 files changed, 735 insertions(+), 1 deletion(-)
 create mode 100644 arch/x86/boot/compressed/sl_main.c
 create mode 100644 arch/x86/boot/compressed/sl_stub.S

diff --git a/arch/x86/boot/compressed/Makefile b/arch/x86/boot/compressed/Makefile
index 1a089f7f7f79..50f1ff478db0 100644
--- a/arch/x86/boot/compressed/Makefile
+++ b/arch/x86/boot/compressed/Makefile
@@ -91,7 +91,7 @@ vmlinux-objs-$(CONFIG_EFI_STUB) += $(obj)/eboot.o $(obj)/efi_stub_$(BITS).o \
 vmlinux-objs-$(CONFIG_EFI_MIXED) += $(obj)/efi_thunk_$(BITS).o
 
 vmlinux-objs-$(CONFIG_SECURE_LAUNCH_STUB) += $(obj)/early_sha1.o \
-	$(obj)/early_tpm.o
+	$(obj)/early_tpm.o $(obj)/sl_main.o $(obj)/sl_stub.o
 
 # The compressed kernel is built with -fPIC/-fPIE so that a boot loader
 # can place it anywhere in memory and it will still run. However, since
diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S
index 64037895b085..9ddae11fe9a6 100644
--- a/arch/x86/boot/compressed/head_64.S
+++ b/arch/x86/boot/compressed/head_64.S
@@ -248,6 +248,22 @@ ENTRY(efi32_stub_entry)
 ENDPROC(efi32_stub_entry)
 #endif
 
+#ifdef CONFIG_SECURE_LAUNCH_STUB
+ENTRY(sl_stub_entry)
+	/*
+	 * On entry, %ebx has the entry abs offset to sl_stub_entry. To
+	 * find the beginning of where we are loaded, sub off from the
+	 * beginning.
+	 */
+	movl	%ebx, %ebp
+	subl	$(sl_stub_entry - startup_32), %ebp
+
+	/* More room to work in sl_stub in the text section */
+	jmp	sl_stub
+
+ENDPROC(sl_stub_entry)
+#endif
+
 	.code64
 	.org 0x200
 ENTRY(startup_64)
@@ -520,6 +536,23 @@ relocated:
 	shrq	$3, %rcx
 	rep	stosq
 
+#ifdef CONFIG_SECURE_LAUNCH_STUB
+	/*
+	 * Have to do the final early sl stub work in 64b area.
+	 *
+	 * *********** NOTE ***********
+	 *
+	 * Several boot params get used before we get a chance to measure
+	 * them in this call. This is a known issue and we currently don't
+	 * have a solution. One solution might be to set them in the really
+	 * early sl stub asm code but that might not work well.
+	 */
+	pushq	%rsi
+	movq	%rsi, %rdi
+	callq	sl_main
+	popq	%rsi
+#endif
+
 /*
  * Do the extraction, and jump to the new kernel..
  */
diff --git a/arch/x86/boot/compressed/sl_main.c b/arch/x86/boot/compressed/sl_main.c
new file mode 100644
index 000000000000..ac08acc1a232
--- /dev/null
+++ b/arch/x86/boot/compressed/sl_main.c
@@ -0,0 +1,186 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.
+ */
+
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <asm/segment.h>
+#include <asm/boot.h>
+#include <asm/msr.h>
+#include <asm/mtrr.h>
+#include <asm/processor-flags.h>
+#include <asm/asm-offsets.h>
+#include <asm/sha1.h>
+#include <asm/tpm.h>
+#include <asm/bootparam.h>
+#include <asm/slaunch.h>
+
+static u64 sl_txt_read(u32 reg)
+{
+	void *addr = (void *)(u64)(TXT_PRIV_CONFIG_REGS_BASE + reg);
+	u64 val;
+
+	barrier();
+	val = *(u64 *)(addr);
+	/* Memory barrier for MMIO read as done in readb() */
+	rmb();
+
+	return val;
+}
+
+static void sl_txt_write(u32 reg, u64 val)
+{
+	void *addr = (void *)(u64)(TXT_PRIV_CONFIG_REGS_BASE + reg);
+
+	barrier();
+	*(u64 *)(addr) = val;
+	/* Memory barrier for MMIO read as done in readb() */
+	wmb();
+	barrier();
+}
+
+static void sl_txt_reset(u64 error)
+{
+	/* Reading the E2STS register acts as a barrier for TXT registers */
+	sl_txt_write(TXTCR_ERRORCODE, error);
+	sl_txt_read(TXTCR_E2STS);
+	sl_txt_write(TXTCR_CMD_UNLOCK_MEM_CONFIG, 1);
+	sl_txt_read(TXTCR_E2STS);
+	sl_txt_write(TXTCR_CMD_RESET, 1);
+	for ( ; ; )
+		__asm__ __volatile__ ("pause");
+}
+
+static u64 sl_rdmsr(u32 reg)
+{
+	u64 lo, hi;
+
+	__asm__ __volatile__ ("rdmsr"  : "=a" (lo), "=d" (hi) : "c" (reg));
+
+	return (hi << 32) | lo;
+}
+
+static void sl_txt_validate_msrs(struct txt_os_mle_data *os_mle_data)
+{
+#define CAPS_VARIABLE_MTRR_COUNT_MASK   0xff
+	u64 mtrr_caps, mtrr_def_type, mtrr_var, misc_en_msr;
+	u32 vcnt, i;
+	struct txt_mtrr_state *saved_bsp_mtrrs =
+		&(os_mle_data->saved_bsp_mtrrs);
+
+	mtrr_caps = sl_rdmsr(MSR_MTRRcap);
+	vcnt = (u32)(mtrr_caps & CAPS_VARIABLE_MTRR_COUNT_MASK);
+
+	if (saved_bsp_mtrrs->mtrr_vcnt > vcnt)
+		sl_txt_reset(TXT_SLERROR_MTRR_INV_VCNT);
+	if (saved_bsp_mtrrs->mtrr_vcnt > TXT_MAX_VARIABLE_MTRRS)
+		sl_txt_reset(TXT_SLERROR_MTRR_INV_VCNT);
+
+	mtrr_def_type = sl_rdmsr(MSR_MTRRdefType);
+	if (saved_bsp_mtrrs->default_type_reg != mtrr_def_type)
+		sl_txt_reset(TXT_SLERROR_MTRR_INV_DEF_TYPE);
+
+	for (i = 0; i < saved_bsp_mtrrs->mtrr_vcnt; i++) {
+		mtrr_var = sl_rdmsr(MTRRphysBase_MSR(i));
+		if (saved_bsp_mtrrs->mtrr_pair[i].mtrr_physbase != mtrr_var)
+			sl_txt_reset(TXT_SLERROR_MTRR_INV_BASE);
+		mtrr_var = sl_rdmsr(MTRRphysMask_MSR(i));
+		if (saved_bsp_mtrrs->mtrr_pair[i].mtrr_physmask != mtrr_var)
+			sl_txt_reset(TXT_SLERROR_MTRR_INV_MASK);
+	}
+
+	misc_en_msr = sl_rdmsr(MSR_IA32_MISC_ENABLE);
+	if (os_mle_data->saved_misc_enable_msr != misc_en_msr)
+		sl_txt_reset(TXT_SLERROR_MSR_INV_MISC_EN);
+}
+
+void sl_main(u8 *bootparams)
+{
+	struct sha1_state sctx = {0};
+	u8 sha1_hash[SHA1_DIGEST_SIZE];
+	struct tpm *tpm;
+	struct boot_params *bp;
+	struct txt_os_mle_data *os_mle_data;
+	u64 *txt_heap;
+	u64 bios_data_size;
+	u32 os_mle_len;
+	int ret;
+
+	memset(sha1_hash, 0, SHA1_DIGEST_SIZE);
+
+	/*
+	 * If enable_tpm fails there is no point going on. The entire secure
+	 * environment depends on this and the other TPM operations succeeding.
+	 */
+	tpm = enable_tpm();
+	if (!tpm)
+		sl_txt_reset(TXT_SLERROR_TPM_INIT);
+
+	if (tpm_request_locality(tpm, 2) == TPM_NO_LOCALITY)
+		sl_txt_reset(TXT_SLERROR_TPM_GET_LOC);
+
+	/* Measure the zero page/boot params */
+	early_sha1_init(&sctx);
+	early_sha1_update(&sctx, bootparams, PAGE_SIZE);
+	early_sha1_finalize(&sctx);
+	early_sha1_finish(&sctx, &sha1_hash[0]);
+	ret = tpm_extend_pcr(tpm, 18, TPM_HASH_ALG_SHA1, &sha1_hash[0]);
+	if (ret)
+		sl_txt_reset(TXT_SLERROR_TPM_EXTEND);
+
+	/* Now safe to use boot params */
+	bp = (struct boot_params *)bootparams;
+
+	/* Measure the command line */
+	memset(sha1_hash, 0, SHA1_DIGEST_SIZE);
+	early_sha1_init(&sctx);
+	early_sha1_update(&sctx, (u8 *)((u64)bp->hdr.cmd_line_ptr),
+			  bp->hdr.cmdline_size);
+	early_sha1_finalize(&sctx);
+	early_sha1_finish(&sctx, &sha1_hash[0]);
+	ret = tpm_extend_pcr(tpm, 18, TPM_HASH_ALG_SHA1, &sha1_hash[0]);
+	if (ret)
+		sl_txt_reset(TXT_SLERROR_TPM_EXTEND);
+
+	/* Measure any external initrd */
+	if (bp->hdr.ramdisk_image != 0 && bp->hdr.ramdisk_size != 0) {
+		memset(sha1_hash, 0, SHA1_DIGEST_SIZE);
+		early_sha1_init(&sctx);
+		early_sha1_update(&sctx, (u8 *)((u64)bp->hdr.ramdisk_image),
+				  bp->hdr.ramdisk_size);
+		early_sha1_finalize(&sctx);
+		early_sha1_finish(&sctx, &sha1_hash[0]);
+		ret = tpm_extend_pcr(tpm, 17, TPM_HASH_ALG_SHA1, &sha1_hash[0]);
+		if (ret)
+			sl_txt_reset(TXT_SLERROR_TPM_EXTEND);
+	}
+
+	/*
+	 * Some extra work to do on Intel, have to measure the OS-MLE
+	 * heap area.
+	 */
+	txt_heap = (void*)sl_txt_read(TXTCR_HEAP_BASE);
+	bios_data_size = *txt_heap;
+	os_mle_data = (struct txt_os_mle_data *)
+			((u8*)txt_heap + bios_data_size + sizeof(u64));
+
+	/* Measure OS-MLE data up to the TPM log into 18 */
+	os_mle_len = offsetof(struct txt_os_mle_data, event_log_buffer);
+	early_sha1_init(&sctx);
+	early_sha1_update(&sctx, (u8 *)os_mle_data, os_mle_len);
+	early_sha1_finalize(&sctx);
+	early_sha1_finish(&sctx, &sha1_hash[0]);
+	ret = tpm_extend_pcr(tpm, 18, TPM_HASH_ALG_SHA1, &sha1_hash[0]);
+	if (ret)
+		sl_txt_reset(TXT_SLERROR_TPM_EXTEND);
+
+	/*
+	 * Now that the OS-MLE data is measured, ensure the MTRR and
+	 * misc enable MSRs are what we expect.
+	 */
+	sl_txt_validate_msrs(os_mle_data);
+
+	tpm_relinquish_locality(tpm);
+	free_tpm(tpm);
+}
diff --git a/arch/x86/boot/compressed/sl_stub.S b/arch/x86/boot/compressed/sl_stub.S
new file mode 100644
index 000000000000..4bf34801418e
--- /dev/null
+++ b/arch/x86/boot/compressed/sl_stub.S
@@ -0,0 +1,505 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+
+/*
+ * Copyright (c) 2019 Oracle and/or its affiliates. All rights reserved.
+ *
+ * Author(s):
+ *     Ross Philipson <ross.philipson@oracle.com>
+ */
+	.code32
+	.text
+#include <linux/linkage.h>
+#include <asm/segment.h>
+#include <asm/msr.h>
+#include <asm/processor-flags.h>
+#include <asm/asm-offsets.h>
+#include <asm/bootparam.h>
+#include <asm/irq_vectors.h>
+#include <asm/slaunch.h>
+
+/* Can't include apiddef.h in asm */
+#define APIC_BASE_MSR	0x800
+#define XAPIC_ENABLE	(1 << 11)
+#define X2APIC_ENABLE	(1 << 10)
+#define	APIC_EOI	0xB0
+#define	APIC_EOI_ACK	0x0
+
+/* Can't include traps.h in asm */
+#define X86_TRAP_NMI	2
+
+/* Can't include mtrr.h in asm */
+#define MTRRphysBase0	0x200
+
+	/* The MLE Header per the TXT Specification, section 4.1 */
+	.global	sl_mle_header
+sl_mle_header:
+	.long	0x9082ac5a    /* UUID0 */
+	.long	0x74a7476f    /* UUID1 */
+	.long	0xa2555c0f    /* UUID2 */
+	.long	0x42b651cb    /* UUID3 */
+	.long	0x00000034    /* MLE header size */
+	.long	0x00020002    /* MLE version 2.2 */
+	.long	sl_stub_entry /* Linear entry point of MLE (virt. address) */
+	.long	0x00000000    /* First valid page of MLE */
+	.long	0x00000000    /* Offset within binary of first byte of MLE */
+	.long	0x00000000    /* Offset within binary of last byte + 1 of MLE */
+	.long	0x00000223    /* Bit vector of MLE-supported capabilities */
+	.long	0x00000000    /* Starting linear address of command line */
+	.long	0x00000000    /* Ending linear address of command line */
+
+	.code32
+ENTRY(sl_stub)
+	/*
+	 * On entry, %ebp has the base address from head_64.S
+	 * and only %cs is known good
+	 */
+	cli
+	cld
+
+	/*
+	 * Take the first stack for the BSP. The AP stacks are only used
+	 * on Intel.
+	 */
+	leal	sl_stacks_end(%ebp), %esp
+
+	/* Load GDT, set segment regs and lret to __SL32_CS */
+	addl	%ebp, (sl_gdt_desc + 2)(%ebp)
+	lgdt	sl_gdt_desc(%ebp)
+
+	movl	$(__SL32_DS), %eax
+	movw	%ax, %ds
+	movw	%ax, %es
+	movw	%ax, %fs
+	movw	%ax, %gs
+	movw	%ax, %ss
+
+	leal	.Lsl_cs(%ebp), %eax
+	pushl	$(__SL32_CS)
+	pushl	%eax
+	lret
+
+.Lsl_cs:
+	/* Before going any further, make sure this is the BSP */
+	movl	$(MSR_IA32_APICBASE), %ecx
+	rdmsr
+	testl	$(MSR_IA32_APICBASE_BSP), %eax
+	jnz	.Lbsp_ok
+	ud2
+
+.Lbsp_ok:
+	/* Assume CPU is AMD to start */
+	movl	$(SL_CPU_AMD), %edi
+
+	/* Now see if it is GenuineIntel. CPUID 0 returns the manufacturer */
+	xorl	%eax, %eax
+	cpuid
+	cmpl	$(INTEL_CPUID_MFGID_EBX), %ebx
+	jnz	.Ldo_amd
+	cmpl	$(INTEL_CPUID_MFGID_EDX), %edx
+	jnz	.Ldo_amd
+	cmpl	$(INTEL_CPUID_MFGID_ECX), %ecx
+	jnz	.Ldo_amd
+	movl	$(SL_CPU_INTEL), %edi
+
+	/* Know it is Intel */
+	movl	$(SL_CPU_INTEL), sl_cpu_type(%ebp)
+
+	/* Increment CPU count for BSP */
+	incl	sl_txt_cpu_count(%ebp)
+
+	/* Enable SMI with GETSET[SMCTRL] */
+	xorl	%ebx, %ebx
+	movl	$(SMX_X86_GETSEC_SMCTRL), %eax
+	.byte 	0x0f, 0x37 /* GETSEC opcode */
+
+	/* IRET-to-self can be used to enable NMIs which SENTER disabled */
+	leal	.Lnmi_enabled(%ebp), %eax
+	pushfl
+	pushl	$(__SL32_CS)
+	pushl	%eax
+	iret
+
+.Lnmi_enabled:
+	/* Clear the TXT error registers for a clean start of day */
+	movl	$0, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_ERRORCODE)
+	movl	$0xffffffff, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_ESTS)
+
+	/* On Intel, the zero page address is passed in the TXT heap */
+	/* Read physical base of heap into EAX */
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_HEAP_BASE), %eax
+	/* Read the size of the BIOS data into ECX (in first 8 bytes */
+	movl	(%eax), %ecx
+	/* Skip over BIOS data and size of OS to MLE */
+	addl	%ecx, %eax
+	addl	$8, %eax
+	/* First 4 bytes of OS to MLE are the version */
+	/* Second 4 bytes of OS to MLE are the zero page */
+	movl	SL_zero_page_addr(%eax), %esi
+
+	/* Save ebp so the APs can find their way home */
+	movl	%ebp, SL_ap_wake_ebp(%eax)
+
+	/* Store the AP PM entry address location that is update later */
+	leal	sl_ap_pm_entry_addr(%ebp), %ecx
+	movl	%ecx, SL_ap_pm_entry(%eax)
+
+	/* Note only %esi and %ebp MUST be preserved across calls */
+	movl	%eax, %edi
+	call	sl_txt_load_regs
+
+	/* Wake up all APs and wait for them to halt */
+	call	sl_txt_wake_aps
+
+	jmp	.Lcpu_setup_done
+
+.Ldo_amd:
+	/* AMD is not yet supported */
+	ud2
+
+	/* On AMD %esi is set up by the Landing Zone, just go on */
+
+.Lcpu_setup_done:
+	/*
+	 * Don't enable MCE at this point. The kernel will enable
+	 * it on the BSP later when it is ready.
+	 */
+
+	/* Keep SL segments for the early portion of the kernel boot */
+	orb	$(KEEP_SEGMENTS), BP_loadflags(%esi)
+
+	/* Done, jump to normal 32b pm entry */
+	jmp	startup_32
+ENDPROC(sl_stub)
+
+ENTRY(sl_txt_ap_entry)
+	cli
+	cld
+
+	/*
+	 * The code segment is known good. The data segments are
+	 * fine too so we can get to our stack before loading the
+	 * GDT.
+	 *
+	 * First order of business is to find where we are and
+	 * save it in ebp.
+	 */
+
+	/* Read physical base of heap into EAX */
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_HEAP_BASE), %eax
+	/* Read the size of the BIOS data into ECX (in first 8 bytes */
+	movl	(%eax), %ecx
+	/* Skip over BIOS data and size of OS to MLE */
+	addl	%ecx, %eax
+	addl	$8, %eax
+
+	/* Saved ebp from the BSP and stash OS-MLE pointer */
+	movl	SL_ap_wake_ebp(%eax), %ebp
+	movl	%eax, %edi
+
+	/* Lock and get our stack index */
+	movl	$1, %ecx
+.Lspin:
+	xorl	%eax, %eax
+	lock cmpxchgl	%ecx, sl_txt_spin_lock(%ebp)
+	jnz	.Lspin
+
+	leal	sl_txt_stack_index(%ebp), %ebx
+	movl	(%ebx), %eax
+	incl	%eax
+	movl	%eax, (%ebx)
+
+	/* Unlock */
+	movl	$0, sl_txt_spin_lock(%ebp)
+
+	/* Load our AP stack */
+	movl	$(TXT_BOOT_STACK_SIZE), %edx
+	mull	%edx
+	leal	sl_stacks_end(%ebp), %edx
+	subl	%eax, %edx
+	movl	%edx, %esp
+
+	/* Load GDT, set segment regs and lret to __SL32_CS */
+	lgdt	sl_gdt_desc(%ebp)
+
+	movl	$(__SL32_DS), %eax
+	movw	%ax, %ds
+	movw	%ax, %es
+	movw	%ax, %fs
+	movw	%ax, %gs
+	movw	%ax, %ss
+
+	leal	.Lsl_ap_cs(%ebp), %eax
+	pushl	$(__SL32_CS)
+	pushl	%eax
+	lret
+
+.Lsl_ap_cs:
+	/* Load the IDT */
+	lidt	sl_idt_desc(%ebp)
+
+	/* Enable SMI with GETSET[SMCTRL] */
+	xorl	%ebx, %ebx
+	movl	$(SMX_X86_GETSEC_SMCTRL), %eax
+	.byte 	0x0f, 0x37 /* GETSEC opcode */
+
+	/* IRET-to-self can be used to enable NMIs which SENTER disabled */
+	leal	.Lnmi_enabled_ap(%ebp), %eax
+	pushfl
+	pushl	$(__SL32_CS)
+	pushl	%eax
+	iret
+
+.Lnmi_enabled_ap:
+	/* Fixup MTRRs and misc enable MSR on APs too */
+	call	sl_txt_load_regs
+
+	/* Put APs in X2APIC mode like the BSP */
+	movl	$(MSR_IA32_APICBASE), %ecx
+	rdmsr
+	movl	%eax, %edi
+	orl	$(XAPIC_ENABLE|X2APIC_ENABLE), %eax
+	wrmsr
+
+	/*
+	 * Basically done, increment the CPU count and wait for NMI IPI.
+	 * Leave all other interrupts masked since we do no expect anything
+	 * but an NMI.
+	 */
+	xorl	%ebx, %ebx
+	lock incl	sl_txt_cpu_count(%ebp)
+
+1:
+	cmpl	$0, %ebx
+	jnz	2f
+	pause
+	jmp	1b
+2:
+
+	.byte	0xea
+sl_ap_pm_entry_addr:
+	.long	0x00000000
+	.word	__SL32_CS
+ENDPROC(sl_txt_ap_entry)
+
+ENTRY(sl_txt_load_regs)
+	/*
+	 * On Intel, the original variable MTRRs and Misc Enable MSR are
+	 * restored on the BSP at early boot. Each AP will also restore
+	 * its MTRRs and Misc Enable MSR.
+	 */
+	pushl	%edi
+	addl	$(SL_saved_bsp_mtrrs), %edi
+	movl	(%edi), %ebx
+	pushl	%ebx /* default_type_reg lo */
+	addl	$4, %edi
+	movl	(%edi), %ebx
+	pushl	%ebx /* default_type_reg hi */
+	addl	$4, %edi
+	movl	(%edi), %ebx /* mtrr_vcnt lo, don't care about hi part */
+	addl	$8, %edi /* now at MTRR pair array */
+	/* Write the variable MTRRs */
+	movl	$(MTRRphysBase0), %ecx
+1:
+	cmpl	$0, %ebx
+	jz	2f
+
+	movl	(%edi), %eax /* MTRRphysBaseX lo */
+	addl	$4, %edi
+	movl	(%edi), %edx /* MTRRphysBaseX hi */
+	wrmsr
+	addl	$4, %edi
+	incl	%ecx
+	movl	(%edi), %eax /* MTRRphysMaskX lo */
+	addl	$4, %edi
+	movl	(%edi), %edx /* MTRRphysMaskX hi */
+	wrmsr
+	addl	$4, %edi
+	incl	%ecx
+
+	decl	%ebx
+	jmp	1b
+2:
+	/* Write the default MTRR register */
+	popl	%edx
+	popl	%eax
+	movl	$(MSR_MTRRdefType), %ecx
+	wrmsr
+
+	/* Return to beginning and write the misc enable msr */
+	popl	%edi
+	addl	$(SL_saved_misc_enable_msr), %edi
+	movl	(%edi), %eax /* saved_misc_enable_msr lo */
+	addl	$4, %edi
+	movl	(%edi), %edx /* saved_misc_enable_msr hi */
+	movl	$(MSR_IA32_MISC_ENABLE), %ecx
+	wrmsr
+
+	ret
+ENDPROC(sl_txt_load_regs)
+
+ENTRY(sl_txt_wake_aps)
+	/* First setup the IDT for the APs to use */
+	leal	sl_txt_int_reset(%ebp), %ebx
+	leal	sl_idt(%ebp), %ecx
+	xorl	%edx, %edx
+
+1:
+	cmpw	$(NR_VECTORS), %dx
+	jz	.Lidt_done
+
+	cmpw	$(X86_TRAP_NMI), %dx
+	jz	2f
+
+	/* Load all other fixed vectors with reset handler (SNO) */
+	movl	%ebx, %eax
+	movw	%ax, (%ecx)
+	shrl	$16, %eax
+	movw	%ax, 6(%ecx)
+	jmp	3f
+
+2:
+	/* Load single wake NMI IPI vector */
+	leal	sl_txt_int_ipi_wake(%ebp), %eax
+	movw	%ax, (%ecx)
+	shrl	$16, %eax
+	movw	%ax, 6(%ecx)
+
+3:
+	incw	%dx
+	addl	$8, %ecx
+	jmp	1b
+
+.Lidt_done:
+	/* Fixup descriptor */
+	addl	%ebp, (sl_idt_desc + 2)(%ebp)
+
+	/* Next setup the MLE join structure and load it into TXT reg */
+	leal	sl_gdt(%ebp), %eax
+	leal	sl_txt_ap_entry(%ebp), %ecx
+	leal	sl_txt_mle_join(%ebp), %edx
+	movl	%eax, 4(%edx)
+	movl	%ecx, 12(%edx)
+	movl	%edx, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_MLE_JOIN)
+
+	/* Another TXT heap walk to find various values needed to wake APs */
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_HEAP_BASE), %eax
+	/* At BIOS data size, find the number of logical processors */
+	movl	(TXT_BIOS_NUM_LOG_PROCS + 8)(%eax), %edx
+	/* Skip over BIOS data */
+	movl	(%eax), %ecx
+	addl	%ecx, %eax
+	/* Skip over OS to MLE */
+	movl	(%eax), %ecx
+	addl	%ecx, %eax
+	/* At OS-SNIT size, get capabilities to know how to wake up the APs */
+	movl	(TXT_OS_SINIT_CAPABILITIES + 8)(%eax), %ebx
+	/* Skip over OS to SNIT */
+	movl	(%eax), %ecx
+	addl	%ecx, %eax
+	/* At SNIT-MLE size, get the AP wake MONITOR address */
+	movl	(TXT_SINIT_MLE_RLP_WAKEUP_ADDR + 8)(%eax), %edi
+
+	/* Determine how to wake up the APs */
+	testl	$(1 << TXT_SINIT_MLE_CAP_WAKE_MONITOR), %ebx
+	jz	.Lwake_getsec
+
+	/* Wake using MWAIT MONITOR */
+	movl	$1, (%edi)
+	jmp	.Laps_awake
+
+.Lwake_getsec:
+	/* Wake using GETSEC(WAKEUP) */
+	xorl	%ebx, %ebx
+	movl	$(SMX_X86_GETSEC_WAKEUP), %eax
+	.byte 	0x0f, 0x37 /* GETSEC opcode */
+
+.Laps_awake:
+	/* Wait for all of them to halt */
+1:
+	cmpl	sl_txt_cpu_count(%ebp), %edx
+	jz	2f
+	pause
+	jmp	1b
+
+2:
+	ret
+ENDPROC(sl_txt_wake_aps)
+
+ENTRY(sl_txt_int_ipi_wake)
+	movl	$1, %ebx
+
+	movl	$(APIC_EOI), %ecx
+	shrl	$4, %ecx
+	addl	$(APIC_BASE_MSR), %ecx
+	movl	$(APIC_EOI_ACK), %eax
+	wrmsr
+
+	iret
+ENDPROC(sl_txt_int_ipi_wake)
+
+ENTRY(sl_txt_int_reset)
+	movl	$(TXT_SLERROR_INV_AP_INTERRUPT), (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_ERRORCODE)
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_E2STS), %eax
+	movl	$1, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_CMD_UNLOCK_MEM_CONFIG)
+	movl	(TXT_PRIV_CONFIG_REGS_BASE + TXTCR_E2STS), %eax
+	movl	$1, (TXT_PRIV_CONFIG_REGS_BASE + TXTCR_CMD_RESET)
+1:
+	pause
+	jmp 	1b
+ENDPROC(sl_txt_int_reset)
+
+	.data
+	.balign 4096
+sl_gdt_desc:
+	.word	sl_gdt_end - sl_gdt - 1
+	.long	sl_gdt
+sl_gdt_desc_end:
+
+	.balign	16
+sl_gdt:
+	.quad	0x0000000000000000	/* NULL */
+	.quad	0x00cf9a000000ffff	/* __SL32_CS */
+	.quad	0x00cf92000000ffff	/* __SL32_DS */
+sl_gdt_end:
+
+	.balign 16
+sl_idt_desc:
+	.word	sl_idt_end - sl_idt - 1	/* Limit */
+	.long	sl_idt			/* Base */
+sl_idt_desc_end:
+
+	.balign 16
+sl_idt:
+	.rept	NR_VECTORS
+	.word	0x0000		/* Offset 15 to 0 */
+	.word	__SL32_CS	/* Segment selector */
+	.word	0x8e00		/* Present, DPL=0, 32b Vector, Interrupt */
+	.word	0x0000		/* Offset 31 to 16 */
+	.endr
+sl_idt_end:
+
+	.balign 16
+sl_txt_mle_join:
+	.long	sl_gdt_end - sl_gdt - 1	/* GDT limit */
+	.long	0x00000000		/* GDT base */
+	.long	__SL32_CS	/* Seg Sel - CS (DS, ES, SS = seg_sel+8) */
+	.long	0x00000000	/* Entry point physical address */
+
+	.global	sl_cpu_type
+sl_cpu_type:
+	.long	0x00000000
+
+sl_txt_spin_lock:
+	.long	0x00000000
+
+sl_txt_stack_index:
+	.long	0x00000000
+
+sl_txt_cpu_count:
+	.long	0x00000000
+
+	/* Small stacks for BSP and APs to work with */
+	.balign 4
+sl_stacks:
+	.fill (TXT_MAX_CPUS*TXT_BOOT_STACK_SIZE), 1, 0
+sl_stacks_end:
diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c
index 01de31db300d..12cbca8a4f95 100644
--- a/arch/x86/kernel/asm-offsets.c
+++ b/arch/x86/kernel/asm-offsets.c
@@ -18,6 +18,7 @@
 #include <asm/bootparam.h>
 #include <asm/suspend.h>
 #include <asm/tlbflush.h>
+#include <asm/slaunch.h>
 
 #ifdef CONFIG_XEN
 #include <xen/interface/xen.h>
@@ -108,4 +109,13 @@ void common(void) {
 	/* Offset for sp0 and sp1 into the tss_struct */
 	OFFSET(TSS_sp0, tss_struct, x86_tss.sp0);
 	OFFSET(TSS_sp1, tss_struct, x86_tss.sp1);
+
+#ifdef CONFIG_SECURE_LAUNCH_STUB
+	BLANK();
+	OFFSET(SL_zero_page_addr, txt_os_mle_data, zero_page_addr);
+	OFFSET(SL_saved_misc_enable_msr, txt_os_mle_data, saved_misc_enable_msr);
+	OFFSET(SL_saved_bsp_mtrrs, txt_os_mle_data, saved_bsp_mtrrs);
+	OFFSET(SL_ap_wake_ebp, txt_os_mle_data, ap_wake_ebp);
+	OFFSET(SL_ap_pm_entry, txt_os_mle_data, ap_pm_entry);
+#endif
 }
-- 
2.13.6

